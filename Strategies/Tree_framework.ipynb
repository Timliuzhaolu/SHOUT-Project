{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('messages_labels.csv')\n",
    "multiclass_df = df[['encoded_message', 'convo_stage']]\n",
    "multiclass_df.columns = ['text', 'labels']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(multiclass_df, test_size=0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel(\n",
    "    'longformer',\n",
    "    'multiclass_cls_param/',\n",
    "    num_labels=6\n",
    ") \n",
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test, acc=accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guid = []\n",
    "text = []\n",
    "label = []\n",
    "for i in range(len(wrong_predictions)):\n",
    "    guid.append(wrong_predictions[i].guid)\n",
    "    text.append(wrong_predictions[i].text_a)\n",
    "    label.append(wrong_predictions[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'guid': guid, 'text': text, 'label': label}  \n",
    "wrong_pred_df = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outputs = []\n",
    "probs = []\n",
    "preds = []\n",
    "second_preds = []\n",
    "first_probs = []\n",
    "second_probs = []\n",
    "\n",
    "for i in wrong_pred_df[\"guid\"]: \n",
    "    raw_output = model_outputs[i]\n",
    "    prob = softmax(raw_output)\n",
    "    indices = heapq.nlargest(2, range(len(prob)), key=prob.__getitem__)\n",
    "    first_index = indices[0]\n",
    "    second_index = indices[1]\n",
    "    first_prob = prob[first_index]\n",
    "    second_prob = prob[second_index]\n",
    "    \n",
    "    raw_outputs.append(list(raw_output))\n",
    "    probs.append(list(prob))\n",
    "    preds.append(first_index)\n",
    "    second_preds.append(second_index)\n",
    "    first_probs.append(first_prob)\n",
    "    second_probs.append(second_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[\"raw_outputs\"] = raw_outputs\n",
    "wrong_pred_df[\"probabilities\"] = probs\n",
    "wrong_pred_df[\"predictions\"] = preds\n",
    "wrong_pred_df[\"first_probs\"] = first_probs\n",
    "wrong_pred_df[\"second_preds\"] = second_preds\n",
    "wrong_pred_df[\"second_probs\"] = second_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 2]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### label 2 is wrongly identified as 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 3]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### label 3 is wrongly identified as 4 and 2 with equal amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 4]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### label 4 is wrongly identified as 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 5]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "almost_right = []\n",
    "diffs = []\n",
    "for i in range(wrong_pred_df.shape[0]):\n",
    "    if wrong_pred_df.iloc[i].label == wrong_pred_df.iloc[i].second_preds:\n",
    "        almost_right.append(1)\n",
    "        diff = wrong_pred_df.iloc[i].first_probs - wrong_pred_df.iloc[i].second_probs\n",
    "        diffs.append(diff)\n",
    "        \n",
    "    else:\n",
    "        almost_right.append(0)\n",
    "        diffs.append(\"-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[\"almost_right\"] = almost_right\n",
    "wrong_pred_df[\"differences\"] = diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### label 5 is wrongly identified as 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[\"differences\"] = wrong_pred_df[\"differences\"].astype(float)\n",
    "# num of almost almost right predictions\n",
    "wrong_pred_df[(wrong_pred_df[\"differences\"] < 0.2) & (wrong_pred_df[\"differences\"] > -1)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df.almost_right.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df.to_csv(\"wrong_pred_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df = pd.read_csv(\"wrong_pred_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 1]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 2]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 3]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 4]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 5]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df[wrong_pred_df[\"label\"] == 0]['predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_pred_df.almost_right.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment without 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('messages_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_df = df[['encoded_message', 'convo_stage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = multiclass_df[multiclass_df['convo_stage'] != 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['text', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df['labels'].replace([5], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs(num_train_epochs = 3, \n",
    "                                overwrite_output_dir = True,\n",
    "                                output_dir = \"multiclass_cls_no4/\",\n",
    "                                max_seq_length = 128,\n",
    "                                eval_batch_size = 4,\n",
    "                                train_batch_size = 4,\n",
    "                                cache_dir = \"multiclass_cls_no4/cache_dir/\",\n",
    "                                learning_rate = 1e-5,\n",
    "                                save_model_every_epoch = False,\n",
    "                                weight_decay = 0.01,\n",
    "                                warmup_ratio = 0.05,\n",
    "                                use_early_stopping = True,\n",
    "                                early_stopping_delta = 0.01,\n",
    "                                early_stopping_metric = \"acc\",\n",
    "                                early_stopping_metric_minimize = False,\n",
    "                                evaluate_during_training = True\n",
    "                               )\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    'longformer',\n",
    "    'MaskedLM/checkpoint_240000',\n",
    "    num_labels=5,\n",
    "    args=model_args\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df = train, eval_df = test, acc=accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(test, acc=accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Only 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('messages_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_df = df[['encoded_message', 'convo_stage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_df.columns = ['text', 'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_df['labels'] = multiclass_df['labels'].replace([0, 1, 2, 3, 5], 0)\n",
    "multiclass_df['labels'] = multiclass_df['labels'].replace([4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(multiclass_df, test_size=0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ClassificationArgs(num_train_epochs = 3, \n",
    "                                overwrite_output_dir = True,\n",
    "                                output_dir = \"multiclass_cls_only4/\",\n",
    "                                max_seq_length = 128,\n",
    "                                eval_batch_size = 4,\n",
    "                                train_batch_size = 4,\n",
    "                                cache_dir = \"multiclass_cls_only4/cache_dir/\",\n",
    "                                learning_rate = 1e-5,\n",
    "                                save_model_every_epoch = False,\n",
    "                                weight_decay = 0.01,\n",
    "                                warmup_ratio = 0.05,\n",
    "                                use_early_stopping = True,\n",
    "                                early_stopping_delta = 0.01,\n",
    "                                early_stopping_metric = \"acc\",\n",
    "                                early_stopping_metric_minimize = False,\n",
    "                                evaluate_during_training = True\n",
    "                               )\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    'longformer',\n",
    "    'MaskedLM/checkpoint_240000',\n",
    "    num_labels=2,\n",
    "    args=model_args\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(train_df = train, eval_df = test, acc=accuracy_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
