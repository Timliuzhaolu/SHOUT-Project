{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    LongformerForMaskedLM,\n",
    "    LongformerTokenizer,\n",
    "    LongformerConfig,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"MaskedLM/longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"MaskedLM/longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained('MaskedLM/longformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"df2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in [\"{{PHONE}}\",\"{{UK_MOBILE}}\",\"{{EMAIL}}\"]:\n",
    "    df2.message = df2.message.str.replace(i,\"[scrubbed]\",regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bot_messages = df2[df2.interaction == \"bot\"]\n",
    "vc=bot_messages.message.value_counts()\n",
    "for k,v in vc.iteritems():\n",
    "    if v<1000:\n",
    "        df2.loc[bot_messages[bot_messages.message == k].index, \"message\"] = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(list(vc[vc>=1000].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df2['message'] = df2.message.str.strip().apply(lambda a:\" \"+a)\n",
    "df2['encoded_message'] = \"[\" + df2.interaction + \"]\" + df2.message\n",
    "encoded_conversations = df2.groupby('conversation_id').encoded_message.agg(\" \".join).apply(\n",
    "    lambda a : f\"{tokenizer.bos_token}{a}{tokenizer.eos_token}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "tqdm.pandas(desc=\"find tokens\")\n",
    "token_lambda = lambda a: re.findall(\"[\\[|\\{]+\\w+[\\]|\\}]+\",a) if type(a)==str else np.nan\n",
    "a = encoded_conversations.progress_apply(token_lambda)\n",
    "\n",
    "from collections import defaultdict\n",
    "freq = defaultdict(lambda : 0)\n",
    "\n",
    "tqdm.pandas()\n",
    "for l in tqdm(a):\n",
    "    for i in l:\n",
    "        freq[i] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_tokens = [i for i,j in freq.items() if j>10000]\n",
    "print(new_tokens)\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('MaskedLM/longformer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
